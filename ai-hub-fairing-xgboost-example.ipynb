{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building, training, and deploying XGBoost model using Kubeflow Fairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Fairing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kubeflow Fairing](https://github.com/kubeflow/fairing) provides a high level python API for machine learning operations such as training, hyper-parameter tuning, deploying models, and online/offline predictions with deployed models. It allows for data scientists to be able to develop their ML training code from within Notebooks or Python files. It makes it trivial to kick off remote execution of training and prediction jobs onto different execution platforms (eg. Kubeflow, Google Cloud ML Engine etc.) without requiring any significant changes to the training code itself.\n",
    "\n",
    "The three major pain points that Kubeflow Fairing tackles are:\n",
    "1. Packaging source code into a container \n",
    "1. Interacting with different remote backends like Kubeflow\n",
    "1. Performing ml workflow tasks like going from training to a deployed model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ames housing value prediction using XGBoost on Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will demonstrate how to use [Kubeflow Fairing](https://github.com/kubeflow/fairing) with XGBoost using the [Kaggle Ames Housing Prices prediction](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/). We will do a detailed walk-through of how to implement, train and deploy/serve the model. You will be able to run the exact same workload on-prem and/or on any cloud provider. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "You can download the dataset from the [Kaggle competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). In order to make it convenient we have uploaded the dataset on GCS\n",
    "\n",
    "```\n",
    "gs://kubeflow-examples-data/ames_dataset/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local python setup\n",
    "Let's install python libs required for this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps = \\\n",
    "\"\"\"\n",
    "pandas\n",
    "joblib\n",
    "numpy\n",
    "xgboost\n",
    "sklearn\n",
    "seldon-core\n",
    "google-cloud-storage\n",
    "\"\"\"\n",
    "with open(\"requirements.txt\", 'w') as f:\n",
    "    f.write(deps)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model and training it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(message)s')\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if gsutil (part of gcloud) sdk is installed\n",
    "res = !which gsutild\n",
    "if len(res)==0:\n",
    "    print(\"Please install gcloud/gsutil by following instructions here \" +\n",
    "          \"https://cloud.google.com/sdk/docs/downloads-interactive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying the dataset to local storage from GCS\n",
    "!gsutil cp -r gs://kubeflow-examples-data/ames_dataset/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(file_name, test_size=0.25):\n",
    "    \"\"\"Read input data and split it into train and test.\"\"\"\n",
    "    data = pd.read_csv(file_name)\n",
    "    data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "    y = data.SalePrice\n",
    "    X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
    "\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X.values,\n",
    "                                                      y.values,\n",
    "                                                      test_size=test_size,\n",
    "                                                      shuffle=False)\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    train_X = imputer.fit_transform(train_X)\n",
    "    test_X = imputer.transform(test_X)\n",
    "\n",
    "    return (train_X, train_y), (test_X, test_y), imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y), imputer = read_input(\"ames_dataset/train.csv\")\n",
    "print(\"Imputer statistics: {} of each column\".format(imputer.strategy))\n",
    "pd.DataFrame(imputer.statistics_).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_X,\n",
    "                train_y,\n",
    "                test_X,\n",
    "                test_y,\n",
    "                n_estimators,\n",
    "                learning_rate):\n",
    "    \"\"\"Train the model using XGBRegressor.\"\"\"\n",
    "    model = XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "\n",
    "    model.fit(train_X,\n",
    "            train_y,\n",
    "            early_stopping_rounds=40,\n",
    "            eval_set=[(test_X, test_y)])\n",
    "\n",
    "    print(\"Best RMSE on eval: %.2f with %d rounds\",\n",
    "               model.best_score,\n",
    "               model.best_iteration+1)\n",
    "    return model\n",
    "\n",
    "def eval_model(model, test_X, test_y):\n",
    "    \"\"\"Evaluate the model performance.\"\"\"\n",
    "    predictions = model.predict(test_X)\n",
    "    logging.info(\"mean_absolute_error=%.2f\", mean_absolute_error(predictions, test_y))\n",
    "\n",
    "def save_model(model, model_file):\n",
    "    \"\"\"Save XGBoost model for serving.\"\"\"\n",
    "    joblib.dump(model, model_file)\n",
    "    logging.info(\"Model export success: %s\", model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a GCS bucket for storing model weights and training artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairing\n",
    "GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "GCS_BUCKET_ID = \"{}-fairing-xgboost-demo\".format(GCP_PROJECT)\n",
    "GCS_BUCKET = \"gs://{}\".format(GCS_BUCKET_ID)\n",
    "!gsutil mb {GCS_BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Basic util functions to copy files to and from GCS\n",
    "def upload_file_to_gcs(bucket_name, source_file_name, destination_file_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_file_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print('File {} uploaded to to gs://{}/{}'.format(\n",
    "        source_file_name,\n",
    "        bucket_name,\n",
    "        destination_file_name)) \n",
    "\n",
    "def download_file_from_gcs(bucket_name, source_file_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(source_file_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    print('File gs://{}/{} downloaded to {}'.format(\n",
    "        bucket_name,\n",
    "        source_file_name,\n",
    "        destination_file_name))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model class with train and predict methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingServe(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_input = \"ames_dataset/train.csv\"\n",
    "        self.n_estimators = 50\n",
    "        self.learning_rate = 0.1\n",
    "        self.model_file = \"trained_ames_model.dat\"\n",
    "        self.trained_model = None\n",
    "\n",
    "    def train(self):\n",
    "        (train_X, train_y), (test_X, test_y), _ = read_input(self.train_input)\n",
    "        model = train_model(train_X,\n",
    "                          train_y,\n",
    "                          test_X,\n",
    "                          test_y,\n",
    "                          self.n_estimators,\n",
    "                          self.learning_rate)\n",
    "\n",
    "        eval_model(model, test_X, test_y)\n",
    "        save_model(model, self.model_file)\n",
    "        upload_file_to_gcs(GCS_BUCKET_ID, self.model_file, self.model_file)\n",
    "\n",
    "    def predict(self, X, feature_names):\n",
    "        \"\"\"Predict using the model for given ndarray.\"\"\"\n",
    "        if not self.trained_model:\n",
    "            self.trained_model = joblib.load(self.model_file)\n",
    "        prediction = self.trained_model.predict(data=X)\n",
    "        return [[prediction.item(0), prediction.item(0)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HousingServe().train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Deploying in Fairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up base container and builder for fairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up google container repositories (GCR) for storing output containers. You can use any docker container registry istead of GCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKER_REGISTRY = 'gcr.io/{}/fairing-job'.format(GCP_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_version = \".\".join([str(x) for x in sys.version_info[0:3]])\n",
    "base_image = \"python:{}\".format(py_version)\n",
    "fairing.config.set_builder('docker', registry=DOCKER_REGISTRY, base_image=base_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubeflow Setup\n",
    "Please follow the instructions at https://www.kubeflow.org/docs/started/getting-started/ to create a Kubeflow cluster if you already don't have one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training in KF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you are taking the model class you used in the training locally and passing it to the fairing library along with dependencies like requirements.txt and dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fairing.config.set_deployer('job')\n",
    "fairing.config.set_preprocessor(\"function\", function_obj=HousingServe,\n",
    "                                input_files=[\"requirements.txt\", \"ames_dataset/train.csv\"])\n",
    "fairing.config.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training in Google cloud ML Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porting training from Kubeflow to Google cloud ML engine is just a matter of chaning the deployer to gcp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairing.config.set_deployer('gcp')\n",
    "fairing.config.set_preprocessor(\"function\", function_obj=HousingServe,\n",
    "                                input_files=[\"requirements.txt\", \"ames_dataset/train.csv\"])\n",
    "fairing.config.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying model and creating an endpoint in KF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained a model and we want to deploy it to an online prediction endpoint. This is achived by using serving deployer that creates Kubernetes service for your model. Here the same model class that is used for training is passed that has a predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fairing.config.set_preprocessor(\"function\", function_obj=HousingServe,\n",
    "                                input_files=[\"requirements.txt\", \"trained_ames_model.dat\"])\n",
    "fairing.config.set_deployer('serving', serving_class=\"HousingServe\")\n",
    "fairing.config.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making prediction calls against the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the prediction endpoint from prev step\n",
    "!curl http://<ip-address>:5000/predict -H \"Content-Type: application/x-www-form-urlencoded\" -d 'json={\"data\":{\"tensor\":{\"shape\":[1,37],\"values\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37]}}}'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
